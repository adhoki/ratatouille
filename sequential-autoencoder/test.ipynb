{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import collections\n",
    "from tqdm.notebook import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = collections.defaultdict(int)\n",
    "\n",
    "f = open('/home/ubuntu/recipe-dataset/json/cleaned_layers.json', 'r')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec989a00ce845e392bca1e96e3dd577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1029720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for id in tqdm(data, total=len(data)):\n",
    "    recipe = data[id]\n",
    "\n",
    "    title = re.sub(r'[^\\w\\s]',' ',recipe['title'].lower())\n",
    "    for word in title.split(' '):\n",
    "        vocab[word] += 1\n",
    "    \n",
    "    for instruction in recipe['instructions']:\n",
    "        instruction = re.sub(r'[^\\w\\s]',' ',instruction.lower())\n",
    "        for word in instruction.split(' '):\n",
    "            vocab[word] += 1\n",
    "    \n",
    "    for ingredient in recipe['ingredients']:\n",
    "        ingredient = re.sub(r'[^\\w\\s]',' ',ingredient.lower())\n",
    "        for word in ingredient.split(' '):\n",
    "            vocab[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 151344\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1514/1514 [01:12<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to /home/ubuntu/recipe-dataset/json/vocab_bert.pkl\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from models.bert_encoder import BERTEncoder\n",
    "bert_encoder = BERTEncoder(BertModel.from_pretrained('bert-base-uncased'), BertTokenizer.from_pretrained('bert-base-uncased'), device=device)\n",
    "\n",
    "bert_encoder.run('/home/ubuntu/recipe-dataset/json/vocab.pkl', '/home/ubuntu/recipe-dataset/json/vocab_bert.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "partition = 'test'\n",
    "ids = pickle.load(open('/home/ubuntu/recipe-dataset/test/test_keys.pkl', 'rb'))\n",
    "data = json.load(open('/home/ubuntu/recipe-dataset/json/cleaned_layers.json', 'r'))\n",
    "new_data = {}\n",
    "for i, (id, sample) in enumerate(data.items()):\n",
    "    if sample['partition'] == partition:\n",
    "        new_data[id] = sample\n",
    "\n",
    "data = new_data\n",
    "del new_data\n",
    "\n",
    "image_map = json.load(open('/home/ubuntu/recipe-dataset/json/image_map.json', 'r'))\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random_embedding = torch.randn(768)\n",
    "\n",
    "bert_embeddings = pickle.load(open('/home/ubuntu/recipe-dataset/json/vocab_bert.pkl', 'rb'))\n",
    "bert_embeddings = {k: torch.tensor(v) for k, v in bert_embeddings.items()}\n",
    "ingredient_vocabulary = pickle.load(open('/home/ubuntu/recipe-dataset/json/ingredient_vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 5\n",
    "sample = data[ids[id]]\n",
    "\n",
    "image_ids = image_map[ids[id]]\n",
    "image_id = np.random.choice(image_ids)\n",
    "dataset_images = '/home/ubuntu/recipe-dataset/test/'\n",
    "image_path = dataset_images +'/'.join(list(image_id[:4])) + '/' + image_id\n",
    "\n",
    "title = sample['title']\n",
    "ingredients = sample['ingredients']\n",
    "instructions = sample['instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embedding = [bert_embeddings.get(word, random_embedding) for word in title.lower().split(' ')]\n",
    "\n",
    "instruction_embedding = []\n",
    "for instruction in instructions:\n",
    "    temp = []\n",
    "    instruction = re.sub(r\"[^a-zA-Z0-9]\", \" \", instruction.lower())\n",
    "    print(f\"instruction: {instruction}\")\n",
    "    for word in instruction.strip().split(' '):\n",
    "        e = bert_embeddings.get(word, random_embedding)\n",
    "        temp.append(e)\n",
    "    instruction_embedding.append(torch.cat(temp))\n",
    "\n",
    "# ingredient embeddings contain an additional lookup in the ingredient vocabulary\n",
    "ingredient_embedding = []\n",
    "for ingredient in ingredients:\n",
    "    temp = []\n",
    "    ingredient = re.sub(r\"[^a-zA-Z0-9]\", \" \", ingredient.lower())\n",
    "    for word in ingredient.split(\" \"):\n",
    "        if word in ingredient_vocabulary['ingredient2stem']:\n",
    "            temp.append(bert_embeddings.get(word, random_embedding))\n",
    "    \n",
    "    ingredient_embedding.append(torch.cat(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class RecipeDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Dataset class for loading the title, cleaned ingredients, instructions, list of images, ID for every recipe in the dataset, based on partition (train, validation, test)\n",
    "    \"\"\"\n",
    "    def __init__(self, partition, ids_pkl, cleaned_layers, image_map, dataset_images, bert_embeddings, ingredient_vocabulary, image_logs='', transform=None, seed=42):\n",
    "        \n",
    "        self.partition = partition\n",
    "        self.data = {}\n",
    "        self.ids = []\n",
    "        self.image_logs = image_logs\n",
    "        self.dataset_images = dataset_images\n",
    "        self.transform = transform\n",
    "        self.seed = seed\n",
    "        self.bert_embeddings_path = bert_embeddings\n",
    "        self.ingredient_vocabulary_path = ingredient_vocabulary\n",
    "\n",
    "        if self.partition not in ['train', 'val', 'test']:\n",
    "            raise ValueError('Partition must be one of train, val, test')\n",
    "        \n",
    "        with open(cleaned_layers, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Loaded {len(data)} recipes from {cleaned_layers}\")\n",
    "        \n",
    "        with open(ids_pkl, 'rb') as f:\n",
    "            self.ids = pickle.load(f)\n",
    "        # remove bad ids from the ids list\n",
    "        remove_count = 0\n",
    "        for id in self.ids:\n",
    "            if id not in data or data[id]['partition'] != self.partition or data[id]['ingredients'] == [] or data[id]['instructions'] == []:\n",
    "                self.ids.remove(id)\n",
    "                remove_count += 1\n",
    "\n",
    "        print(f\"PARTITION: {self.partition}, TOTAL IDS AVAILABLE: {len(self.ids)}\")\n",
    "\n",
    "        # iterate through the data to obtain only samples which are from the partition\n",
    "        for i, (id, sample) in enumerate(data.items()):\n",
    "            if sample['partition'] == self.partition:\n",
    "                self.data[id] = sample\n",
    "        \n",
    "        # memory cleanup\n",
    "        del data\n",
    "\n",
    "        with open(image_map, 'r') as f:\n",
    "            self.image_map = json.load(f)\n",
    "        print(f\"Loaded {len(self.image_map)} image mappings from {image_map}\")\n",
    "\n",
    "        torch.random.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.random_embedding = torch.randn(768).unsqueeze(0)\n",
    "        print('random embedding', self.random_embedding.shape)\n",
    "\n",
    "        with open(self.bert_embeddings_path, 'rb') as f:\n",
    "            self.bert_embeddings = pickle.load(f)\n",
    "            self.bert_embeddings = { k: torch.tensor(v).unsqueeze(0) for k, v in self.bert_embeddings.items() }\n",
    "        with open(self.ingredient_vocabulary_path, 'rb') as f:\n",
    "            self.ingredient_vocabulary = pickle.load(f)\n",
    "            \n",
    "        print(f\"Loaded {len(self.bert_embeddings)} embeddings from {self.bert_embeddings_path}\\nLoaded {len(self.ingredient_vocabulary['ingredients'])} ingredients from {self.ingredient_vocabulary_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print('INDEX CALLED:', index)\n",
    "        id = self.ids[index]\n",
    "        sample = self.data[id]\n",
    "\n",
    "        image_ids = self.image_map[id]\n",
    "\n",
    "        # randomly pick out an image from the list of images if train, else pick the first image\n",
    "        if self.partition == 'train':\n",
    "            image_id = np.random.choice(image_ids)\n",
    "        else:\n",
    "            image_id = image_ids[0]\n",
    "\n",
    "        # create the image path and load the image\n",
    "        image_path = self.dataset_images +'/'.join(list(image_id[:4])) + '/' + image_id\n",
    "\n",
    "        # load image from path\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform is not None:\n",
    "                image = self.transform(image)\n",
    "        except:\n",
    "            raise ValueError(f\"Image not found at path: {image_path}\")\n",
    "\n",
    "        # obtain list of ingredients and instructions\n",
    "        title = sample['title']\n",
    "        ingredients = sample['ingredients']\n",
    "        instructions = sample['instructions']\n",
    "        print(f\"{title}\\n{ingredients}\\n{instructions}\")\n",
    "\n",
    "        # obtain the embeddings for title, ingredients and instructions from BERT\n",
    "        # check against the dictionary saved, if not available, then use the random vector generated at the start\n",
    "\n",
    "        title_embedding = [self.bert_embeddings.get(word, self.random_embedding) for word in title.lower().split(' ')]\n",
    "\n",
    "        instruction_embedding = []\n",
    "        for instruction in instructions:\n",
    "            temp = []\n",
    "            instruction = re.sub(r\"[^a-zA-Z0-9]\", \" \", instruction.strip().lower())\n",
    "            # print(f\"instruction: {instruction}\")\n",
    "            for word in instruction.split():\n",
    "                e = self.bert_embeddings.get(word, self.random_embedding)\n",
    "                temp.append(e)\n",
    "            \n",
    "            instruction_embedding.append(torch.cat(temp, dim=0))\n",
    "\n",
    "        # ingredient embeddings contain an additional lookup in the ingredient vocabulary\n",
    "        ingredient_embedding = []\n",
    "        for ingredient in ingredients:\n",
    "            temp = []\n",
    "            ingredient = re.sub(r\"[^a-zA-Z0-9]\", \" \", ingredient.strip().lower())\n",
    "            for word in ingredient.split(\" \"):\n",
    "                temp.append(self.bert_embeddings.get(word, self.random_embedding))\n",
    "            \n",
    "            ingredient_embedding.append(torch.cat(temp, dim=0))\n",
    "\n",
    "        # convert the list of embeddings to a tensor, with zero padding to cover variable length\n",
    "        title_embedding = torch.nn.utils.rnn.pad_sequence(title_embedding, batch_first=True, padding_value=0)\n",
    "        instruction_embedding = torch.nn.utils.rnn.pad_sequence(instruction_embedding, batch_first=True, padding_value=0)\n",
    "        ingredient_embedding = torch.nn.utils.rnn.pad_sequence(ingredient_embedding, batch_first=True, padding_value=0)\n",
    "\n",
    "        # print(f\"TITLE EMBEDDING: {torch.squeeze(title_embedding).shape}\")\n",
    "        # print(f\"INSTRUCTION EMBEDDING: {instruction_embedding.shape}\")\n",
    "        # print(f\"INGREDIENT EMBEDDING: {ingredient_embedding.shape}\")\n",
    "        \n",
    "        output = {\n",
    "            'id': id,\n",
    "            'image_id': image_id,\n",
    "            'title': title,\n",
    "            'ingredients': ingredients,\n",
    "            'instructions': instructions,\n",
    "            'title_embedding': torch.squeeze(title_embedding),\n",
    "            'ingredient_embedding': ingredient_embedding,\n",
    "            'instruction_embedding': instruction_embedding,\n",
    "            'image': image\n",
    "        }\n",
    "\n",
    "        return output\n",
    "\n",
    "    def visualize_sample(self, index):\n",
    "        output = self.__getitem__(index)\n",
    "        print(f\"ID: {output['id']}\\tImage ID: {output['image_id']}\")\n",
    "        print(f\"Title: {output['title']}\")\n",
    "        print(f\"Ingredients:\")\n",
    "        for ingredient in output['ingredients']:\n",
    "            print(f\"\\t{ingredient}\")\n",
    "        print(f\"Instructions:\")\n",
    "        for instruction in output['instructions']:\n",
    "            print(f\"\\t{instruction}\")\n",
    "\n",
    "        if self.image_logs:\n",
    "            image_path = self.dataset_images + '/'.join(list(output['image_id'][:4])) + '/' + output['image_id']\n",
    "            image_path = os.path.join(self.image_logs, image_path)\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image.save(f\"{self.image_logs}/{output['id']}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1029720 recipes from /home/ubuntu/recipe-dataset/json/cleaned_layers.json\n",
      "PARTITION: test, TOTAL IDS AVAILABLE: 51334\n",
      "Loaded 402760 image mappings from /home/ubuntu/recipe-dataset/json/image_map.json\n",
      "random embedding torch.Size([1, 768])\n",
      "Loaded 151344 embeddings from /home/ubuntu/recipe-dataset/json/vocab_bert.pkl\n",
      "Loaded 9225 ingredients from /home/ubuntu/recipe-dataset/json/ingredient_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset = RecipeDataset(\n",
    "        partition='test',\n",
    "        ids_pkl='/home/ubuntu/recipe-dataset/test/test_keys.pkl', \n",
    "        cleaned_layers='/home/ubuntu/recipe-dataset/json/cleaned_layers.json', \n",
    "        image_map='/home/ubuntu/recipe-dataset/json/image_map.json', \n",
    "        dataset_images='/home/ubuntu/recipe-dataset/test/', \n",
    "        bert_embeddings='/home/ubuntu/recipe-dataset/json/vocab_bert.pkl',\n",
    "        ingredient_vocabulary='/home/ubuntu/recipe-dataset/json/ingredient_vocab.pkl',\n",
    "        image_logs='/home/ubuntu/cooking-cross-modal-retrieval/sequential-autoencoder/logs',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEX CALLED: 0\n",
      "Crunchy Onion Potato Bake\n",
      "['milk', 'water', 'butter', 'mashed potatoes', 'whole kernel corn', 'cheddar cheese', 'French - fried onions']\n",
      "['Preheat oven to 350 degrees Fahrenheit.', 'Spray pan with non stick cooking spray.', 'Heat milk, water and butter to boiling; stir in contents of both pouches of potatoes; let stand one minute.', 'Stir in corn.', 'Spoon half the potato mixture in pan.', 'Sprinkle half each of cheese and onions; top with remaining potatoes.', 'Sprinkle with remaining cheese and onions.', 'Bake 10 to 15 minutes until cheese is melted.', 'Enjoy !']\n",
      "TITLE: Crunchy Onion Potato Bake\n",
      "INGREDIENTS: ['milk', 'water', 'butter', 'mashed potatoes', 'whole kernel corn', 'cheddar cheese', 'French - fried onions']\n",
      "INSTRUCTIONS: ['Preheat oven to 350 degrees Fahrenheit.', 'Spray pan with non stick cooking spray.', 'Heat milk, water and butter to boiling; stir in contents of both pouches of potatoes; let stand one minute.', 'Stir in corn.', 'Spoon half the potato mixture in pan.', 'Sprinkle half each of cheese and onions; top with remaining potatoes.', 'Sprinkle with remaining cheese and onions.', 'Bake 10 to 15 minutes until cheese is melted.', 'Enjoy !']\n",
      "torch.Size([4, 768]) torch.Size([7, 5, 768]) torch.Size([9, 19, 768])\n"
     ]
    }
   ],
   "source": [
    "test_output = dataset.__getitem__(0)\n",
    "\n",
    "title = test_output['title']\n",
    "ingredients = test_output['ingredients']\n",
    "instructions = test_output['instructions']\n",
    "\n",
    "title_embedding = test_output['title_embedding']\n",
    "ingredient_embedding = test_output['ingredient_embedding']\n",
    "instruction_embedding = test_output['instruction_embedding']\n",
    "\n",
    "print(f\"TITLE: {title}\\nINGREDIENTS: {ingredients}\\nINSTRUCTIONS: {instructions}\")\n",
    "print(title_embedding.shape, ingredient_embedding.shape, instruction_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple dataloader and pass the inputs through the model\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate(batch):\n",
    "    title_embeddings, ingredient_embeddings, instruction_embeddings, images = [], [], [], []\n",
    "    ingredient_max_seq, ingredient_max_num, instruction_max_seq, instruction_max_num = 0, 0, 0, 0\n",
    "\n",
    "    for elem in batch:\n",
    "        title_embeddings.append(elem['title_embedding'])\n",
    "\n",
    "        ingredient_max_num = max(ingredient_max_num, elem['ingredient_embedding'].shape[0])\n",
    "        ingredient_max_seq = max(ingredient_max_seq, elem['ingredient_embedding'].shape[1])\n",
    "        ingredient_embeddings.append(elem['ingredient_embedding'].unsqueeze(0))\n",
    "        \n",
    "        instruction_max_num = max(instruction_max_num, elem['instruction_embedding'].shape[0])\n",
    "        instruction_max_seq = max(instruction_max_seq, elem['instruction_embedding'].shape[1])\n",
    "        instruction_embeddings.append(elem['instruction_embedding'].unsqueeze(0))\n",
    "        \n",
    "        images.append(elem['image'])\n",
    "    \n",
    "    # title\n",
    "    title_embeddings = torch.nn.utils.rnn.pad_sequence(title_embeddings, batch_first=True, padding_value=0)\n",
    "\n",
    "    # ingredients\n",
    "    padded_output_size = np.array([1, ingredient_max_num, ingredient_max_seq, 768])\n",
    "    for i, elem in enumerate(ingredient_embeddings):\n",
    "        pad = padded_output_size - np.array(elem.shape)\n",
    "        ingredient_embeddings[i] = F.pad(elem, (0, pad[3], 0, pad[2], 0, pad[1], 0, pad[0]))\n",
    "    ingredient_embeddings = torch.cat(ingredient_embeddings, dim=0)\n",
    "    \n",
    "    # instructions\n",
    "    padded_output_size = np.array([1, instruction_max_num, instruction_max_seq, 768])\n",
    "\n",
    "    for i, elem in enumerate(instruction_embeddings):\n",
    "        pad = padded_output_size - np.array(elem.shape)\n",
    "        instruction_embeddings[i] = F.pad(elem, (0, pad[3], 0, pad[2], 0, pad[1], 0, pad[0]))\n",
    "    instruction_embeddings = torch.cat(instruction_embeddings, dim=0)\n",
    "\n",
    "    # images\n",
    "    images = torch.stack(images, dim=0)\n",
    "    print(images.shape)\n",
    "\n",
    "    print(title_embeddings.shape, ingredient_embeddings.shape, instruction_embeddings.shape)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(f\"Batch: {i}\")\n",
    "    print(batch)\n",
    "    # print(batch['title_embedding'].shape, batch['ingredient_embedding'].shape, batch['instruction_embedding'].shape)\n",
    "    # print(batch['image'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = dataset.__getitem__(12)\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018\n"
     ]
    }
   ],
   "source": [
    "with open('/home/ubuntu/recipe-dataset/json/cleaned_layers.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    count = 0\n",
    "    for id, item in data.items():\n",
    "        title = item['title']\n",
    "        ingredients = item['ingredients']\n",
    "        instructions = item['instructions']\n",
    "\n",
    "        if len(ingredients) == 0 or len(instructions) == 0:\n",
    "            # print(id)\n",
    "            count += 1\n",
    "    \n",
    "    print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
