{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import collections\n",
    "from tqdm.notebook import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = collections.defaultdict(int)\n",
    "\n",
    "f = open('/home/ubuntu/recipe-dataset/json/cleaned_layers.json', 'r')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec989a00ce845e392bca1e96e3dd577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1029720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for id in tqdm(data, total=len(data)):\n",
    "    recipe = data[id]\n",
    "\n",
    "    title = re.sub(r'[^\\w\\s]',' ',recipe['title'].lower())\n",
    "    for word in title.split(' '):\n",
    "        vocab[word] += 1\n",
    "    \n",
    "    for instruction in recipe['instructions']:\n",
    "        instruction = re.sub(r'[^\\w\\s]',' ',instruction.lower())\n",
    "        for word in instruction.split(' '):\n",
    "            vocab[word] += 1\n",
    "    \n",
    "    for ingredient in recipe['ingredients']:\n",
    "        ingredient = re.sub(r'[^\\w\\s]',' ',ingredient.lower())\n",
    "        for word in ingredient.split(' '):\n",
    "            vocab[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 151344\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1514/1514 [01:12<00:00, 20.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to /home/ubuntu/recipe-dataset/json/vocab_bert.pkl\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from models.bert_encoder import BERTEncoder\n",
    "bert_encoder = BERTEncoder(BertModel.from_pretrained('bert-base-uncased'), BertTokenizer.from_pretrained('bert-base-uncased'), device=device)\n",
    "\n",
    "bert_encoder.run('/home/ubuntu/recipe-dataset/json/vocab.pkl', '/home/ubuntu/recipe-dataset/json/vocab_bert.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "partition = 'test'\n",
    "ids = pickle.load(open('/home/ubuntu/recipe-dataset/test/test_keys.pkl', 'rb'))\n",
    "data = json.load(open('/home/ubuntu/recipe-dataset/json/cleaned_layers.json', 'r'))\n",
    "new_data = {}\n",
    "for i, (id, sample) in enumerate(data.items()):\n",
    "    if sample['partition'] == partition:\n",
    "        new_data[id] = sample\n",
    "\n",
    "data = new_data\n",
    "del new_data\n",
    "\n",
    "image_map = json.load(open('/home/ubuntu/recipe-dataset/json/image_map.json', 'r'))\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random_embedding = torch.randn(768)\n",
    "\n",
    "bert_embeddings = pickle.load(open('/home/ubuntu/recipe-dataset/json/vocab_bert.pkl', 'rb'))\n",
    "bert_embeddings = {k: torch.tensor(v) for k, v in bert_embeddings.items()}\n",
    "ingredient_vocabulary = pickle.load(open('/home/ubuntu/recipe-dataset/json/ingredient_vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 5\n",
    "sample = data[ids[id]]\n",
    "\n",
    "image_ids = image_map[ids[id]]\n",
    "image_id = np.random.choice(image_ids)\n",
    "dataset_images = '/home/ubuntu/recipe-dataset/test/'\n",
    "image_path = dataset_images +'/'.join(list(image_id[:4])) + '/' + image_id\n",
    "\n",
    "title = sample['title']\n",
    "ingredients = sample['ingredients']\n",
    "instructions = sample['instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embedding = [bert_embeddings.get(word, random_embedding) for word in title.lower().split(' ')]\n",
    "\n",
    "instruction_embedding = []\n",
    "for instruction in instructions:\n",
    "    temp = []\n",
    "    instruction = re.sub(r\"[^a-zA-Z0-9]\", \" \", instruction.lower())\n",
    "    print(f\"instruction: {instruction}\")\n",
    "    for word in instruction.strip().split(' '):\n",
    "        e = bert_embeddings.get(word, random_embedding)\n",
    "        temp.append(e)\n",
    "    instruction_embedding.append(torch.cat(temp))\n",
    "\n",
    "# ingredient embeddings contain an additional lookup in the ingredient vocabulary\n",
    "ingredient_embedding = []\n",
    "for ingredient in ingredients:\n",
    "    temp = []\n",
    "    ingredient = re.sub(r\"[^a-zA-Z0-9]\", \" \", ingredient.lower())\n",
    "    for word in ingredient.split(\" \"):\n",
    "        if word in ingredient_vocabulary['ingredient2stem']:\n",
    "            temp.append(bert_embeddings.get(word, random_embedding))\n",
    "    \n",
    "    ingredient_embedding.append(torch.cat(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredient_vocabulary['ingredient2stem']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
