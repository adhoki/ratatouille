{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from re import L\n",
    "from dataset import get_loader\n",
    "from models import get_model\n",
    "import torch.backends.cudnn as cudnn\n",
    "from config import get_args\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.utils import load_checkpoint, count_parameters\n",
    "import argparse\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "map_loc = None if torch.cuda.is_available() else 'cpu'\n",
    "random.seed(1234)\n",
    "\n",
    "\n",
    "# load the dataset and dataloader...\n",
    "# current implementation has the list of images to load already selected in the dataset file\n",
    "def load_test_data():\n",
    "    dataloader, dataset = get_loader(\n",
    "        root='/home/ubuntu/recipe-dataset',\n",
    "        batch_size=4,\n",
    "        resize=224,\n",
    "        im_size=224,\n",
    "        augment=False, \n",
    "        split='test',\n",
    "        mode='test',\n",
    "        drop_last=False,\n",
    "        load_actual_data=True\n",
    "    )\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "# manual hack to construct arguments for loading the model\n",
    "def construct_model_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--tf_n_heads', type=int, default=4,\n",
    "                        help='Number of attention heads in Transformer models.')\n",
    "    parser.add_argument('--tf_n_layers', type=int, default=2,\n",
    "                        help='Number of layers in Transformer models.')\n",
    "    parser.add_argument('--hidden_recipe', type=int, default=512,\n",
    "                        help='Embedding dimensionality for recipe representation.')\n",
    "    parser.add_argument('--output_size', type=int, default=1024,\n",
    "                        help='Dimensionality of the output embeddings.')\n",
    "    parser.add_argument('--backbone', type=str, default='resnet50',\n",
    "                        help='backbone for the vision model')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def obtain_features(args, dataset, dataloader, checkpoints_dir, store_dict, device):\n",
    "\n",
    "    vocab_size = len(dataset.get_vocab())\n",
    "    model = get_model(args, vocab_size)\n",
    "    print(\"recipe encoder\", count_parameters(model.text_encoder))\n",
    "    print(\"image encoder\", count_parameters(model.image_encoder))\n",
    "\n",
    "    _, model_dict, _ = load_checkpoint(checkpoints_dir, 'best', map_loc,\n",
    "                                          store_dict)\n",
    "    model.load_state_dict(model_dict, strict=True)\n",
    "    if device != 'cpu' and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    image_features = recipe_features = None\n",
    "    all_image_ids = []\n",
    "\n",
    "    for i, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        image, titles, title_targets, ingrs, ingredient_targets, instrs, instruction_targets, ids = batch\n",
    "        image = image.to(device)\n",
    "        title_targets = title_targets.to(device)\n",
    "        ingredient_targets = ingredient_targets.to(device)\n",
    "        instruction_targets = instruction_targets.to(device)\n",
    "\n",
    "        all_image_ids.extend(ids)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_images = model.image_encoder(image)\n",
    "            image_feat, recipe_feat, projected_feat = model(image, title_targets, ingredient_targets, instruction_targets)\n",
    "        \n",
    "        if image_features is None:\n",
    "            image_features = image_feat.cpu().detach().numpy()\n",
    "            recipe_features = recipe_feat.cpu().detach().numpy()\n",
    "        else:\n",
    "            image_features = np.vstack((image_features, image_feat.cpu().detach().numpy()))\n",
    "            recipe_features = np.vstack((recipe_features, recipe_feat.cpu().detach().numpy()))\n",
    "\n",
    "    print(f\"COMPLETED EXTRACTING FEATURES: FEATURE SIZE: {np.shape(image_features)}, {np.shape(recipe_features)}\")\n",
    "\n",
    "    return all_image_ids, image_features, recipe_features\n",
    "\n",
    "\n",
    "def visualize_results():\n",
    "    pass\n",
    "\n",
    "\n",
    "# take in as input the image and recipe features and corresponding image ids\n",
    "# Do the following: \n",
    "# - Compute average medRank and recall rates for top-1, 5, 10\n",
    "# - For each image - compute the top-10 matching image IDs\n",
    "def run_evaluation(task_type, image_features, recipe_features, image_ids, medr_N):\n",
    "    indexes = np.argsort(image_ids)\n",
    "    image_ids = [image_ids[i] for i in indexes]\n",
    "\n",
    "\n",
    "    n = medr_N\n",
    "    idxs = range(n)\n",
    "\n",
    "    global_ranks = []\n",
    "    global_recall = {1: 0.0, 5:0.0, 10:0.0}\n",
    "    closest_image_dict = {}\n",
    "\n",
    "    # repeat the exercise for 10 times\n",
    "    for i in range(10):\n",
    "        ids = random.sample(range(0, len(image_ids)), n)\n",
    "        image_subfeatures = image_features[ids, :]\n",
    "        recipe_subfeatures = recipe_features[ids, :]\n",
    "        selected_ids = [image_ids[i] for i in ids]\n",
    "\n",
    "        if task_type == 'im2recipe':\n",
    "            similarities = np.dot(image_subfeatures, recipe_subfeatures.T)\n",
    "        elif task_type =='recipe2im':\n",
    "            similarities = np.dot(recipe_subfeatures, image_subfeatures.T)\n",
    "        else:\n",
    "            raise ValueError('Invalid task type')\n",
    "\n",
    "        median_ranks = []\n",
    "        recalls = {1: 0.0, 5:0.0, 10:0.0}\n",
    "\n",
    "        for idx in idxs:\n",
    "            name = selected_ids[idx]\n",
    "\n",
    "            similarity = similarities[idx, :]\n",
    "            sorting = np.argsort(similarity)[::-1].tolist()\n",
    "            pos = sorting.index(idx)\n",
    "\n",
    "            closest_image_dict[name] = {\n",
    "                'top_10_images': [selected_ids[i] for i in sorting[:10]],\n",
    "                'similarity_scores': [str(i) for i in similarity[sorting[:10]]]\n",
    "            }\n",
    "\n",
    "            if pos == 0:\n",
    "                recalls[1] += 1\n",
    "            if pos <= 5:\n",
    "                recalls[5] += 1\n",
    "            if pos <= 10:\n",
    "                recalls[10] += 1\n",
    "            \n",
    "            median_ranks.append(pos)\n",
    "\n",
    "        for k in recalls:\n",
    "            recalls[k] = recalls[k] / n \n",
    "\n",
    "        median = np.median(median_ranks)\n",
    "        for k in recalls:\n",
    "            global_recall[k] += recalls[k]\n",
    "        \n",
    "        global_ranks.append(median)\n",
    "\n",
    "    for k in global_recall:\n",
    "        global_recall[k] = global_recall[k] / 10\n",
    "\n",
    "    print(f\"MEAN MEDIAN: {np.average(global_ranks)}\")\n",
    "    print(f\"RECALL: {global_recall}\")\n",
    "\n",
    "    return np.average(global_ranks), global_recall, closest_image_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data: 1029720\n",
      "10000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset, dataloader = load_test_data()\n",
    "\n",
    "model_args = construct_model_args()\n",
    "checkpoints_dir = '/home/ubuntu/cooking-cross-modal-retrieval/image-to-recipe-transformers-main/checkpoints/r50_ssl'\n",
    "store_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe encoder 39998976\n",
      "image encoder 25606208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef05640ec4924bc087cf1fb74358c63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED EXTRACTING FEATURES: FEATURE SIZE: (), ()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m image_ids, image_features, recipe_features \u001b[39m=\u001b[39m obtain_features(\n\u001b[1;32m      2\u001b[0m     model_args, \n\u001b[1;32m      3\u001b[0m     dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     device\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m task_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mim2recipe\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m average_median, global_recalls, closest_image_dict \u001b[39m=\u001b[39m run_evaluation(task_type, image_features, recipe_features, image_ids, medr_N\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [4], line 126\u001b[0m, in \u001b[0;36mrun_evaluation\u001b[0;34m(task_type, image_features, recipe_features, image_ids, medr_N)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m# repeat the exercise for 10 times\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m     ids \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49msample(\u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39mlen\u001b[39;49m(image_ids)), n)\n\u001b[1;32m    127\u001b[0m     image_subfeatures \u001b[39m=\u001b[39m image_features[ids, :]\n\u001b[1;32m    128\u001b[0m     recipe_subfeatures \u001b[39m=\u001b[39m recipe_features[ids, :]\n",
      "File \u001b[0;32m/usr/lib/python3.8/random.py:363\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    361\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(population)\n\u001b[1;32m    362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m k \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n:\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSample larger than population or is negative\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m result \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m k\n\u001b[1;32m    365\u001b[0m setsize \u001b[39m=\u001b[39m \u001b[39m21\u001b[39m        \u001b[39m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "image_ids, image_features, recipe_features = obtain_features(\n",
    "    model_args, \n",
    "    dataset,\n",
    "    dataloader,\n",
    "    checkpoints_dir,\n",
    "    store_dict,\n",
    "    device\n",
    ")\n",
    "\n",
    "task_type = 'im2recipe'\n",
    "average_median, global_recalls, closest_image_dict = run_evaluation(task_type, image_features, recipe_features, image_ids, medr_N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(closest_image_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTED JSON READING\n",
      "DONE JSON READING\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def create_data():\n",
    "    image_map = {}\n",
    "    layer1 = json.load(open('/home/ubuntu/recipe-dataset/layer1.json', 'r'))\n",
    "    layer2 = json.load(open('/home/ubuntu/recipe-dataset/layer2.json', 'r'))\n",
    "\n",
    "    for sample in tqdm(layer1):\n",
    "        count += 1\n",
    "        if sample['partition'] == 'test':\n",
    "            image_map[sample['id']] = {\n",
    "                'title': sample['title'],\n",
    "                'instructions': [a['text'] for a in sample['instructions']],\n",
    "                'ingredients': [a['text'] for a in sample['ingredients']],\n",
    "                'partition': sample['partition'],\n",
    "                'image_path': layer2[sample['id']]['images']['id']\n",
    "            }\n",
    "    \n",
    "    print(count)\n",
    "    json.dump(image_map, open('/home/ubuntu/recipe-dataset/test-image-map.json', 'w'))\n",
    "\n",
    "def load_image(image_id):\n",
    "    data_root_folder = '/home/ubuntu/recipe-dataset/test'\n",
    "    image_path = os.path.join(data_root_folder, image_id[0], image_id[1], image_id[2], image_id[3], image_id+'.jpg')\n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "print(f\"STARTED JSON READING\")\n",
    "title_json = json.load(open('/home/ubuntu/recipe-dataset/title-map.json', 'r'))\n",
    "print(f\"DONE JSON READING\")\n",
    "\n",
    "def get_title(image_id):\n",
    "    return title_json[image_id]\n",
    "\n",
    "def visualize_results(image_dict, base_image):\n",
    "\n",
    "    selected_image_ids = image_dict[base_image]['top_10_images'][:5]\n",
    "    \n",
    "    fig = plt.figure(figsize=(25, 8))\n",
    "\n",
    "    rows, cols = 1, 6\n",
    "\n",
    "    fig.add_subplot(rows, cols, 1)\n",
    "    image = load_image(base_image)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(get_title(base_image))\n",
    "\n",
    "\n",
    "    for i, image_id in selected_image_ids:\n",
    "        fig.add_subplot(rows, cols, i+2)\n",
    "        image = load_image(image_id)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(get_title(image_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(closest_image_dict, '05096fc3ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for root_dir, cur_dir, files in os.walk('/home/ubuntu/recipe-dataset/test'):\n",
    "    print(files)\n",
    "    count += len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51334\n",
      "['3e233001e2.jpg' '1657f23729.jpg' '3020f58577.jpg' ... 'dd7cdc4b69.jpg'\n",
      " '807de6023c.jpg' 'dd05312b3b.jpg']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/ubuntu/recipe-dataset/img_name.pkl', 'rb') as file:\n",
    "    image_file_names = pickle.load(file)\n",
    "\n",
    "print(len(image_file_names))\n",
    "print(image_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/ubuntu/recipe-dataset/traindata/test.pkl', 'rb') as file:\n",
    "    image_file_names = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
